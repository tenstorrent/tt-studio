# TT-Studio

> TT-Studio is an easy-to-use web interface for running AI models on Tenstorrent hardware.
  It combines TT Inference Server's core packaging setup, containerization,
  and deployment automation with TT-Metal's model execution framework specifically optimized for Tenstorrent hardware.

Important notes:

- TT-Studio requires access to a Tenstorrent AI accelerator for full deployment features
- Alternatively, you can connect just the frontend to a remote API endpoint without direct hardware access
- The platform provides automatic hardware detection and seamless integration with Tenstorrent devices
- Uses containerized deployment through Docker for isolation and easy deployment
- The startup.sh script is deprecated - use `python run.py` for all operations

## Docs

- [Main README](README.md): Complete overview, setup instructions, and quick start guide
- [Setup Guide](docs/run-py-guide.md): Complete installation & configuration using run.py
- [FAQ](docs/FAQ.md): Quick answers to common questions about TT-Studio
- [Model Interface Guide](docs/model-interface.md): Using TT-Studio as AI playground (Chat, Vision, Speech, Images)
- [Troubleshooting Guide](docs/troubleshooting.md): Solutions for common setup and runtime issues
- [Contributing Guide](CONTRIBUTING.md): How to contribute code to the project
- [Development Setup](docs/development.md): Development environment configuration

## Examples

- [AI Model Interface](docs/model-interface.md): Complete examples of using Chat, Vision, Speech, and Image models
- [vLLM Models Guide](docs/HowToRun_vLLM_Models.md): Specific examples for running vLLM models
- [AI Agent Setup](app/agent/README.md): Setting up and using the AI assistant functionality

## Key Components

- **Frontend Interface**: Modern React-based UI for model interaction and management
- **Backend API**: Django-based service for model management, deployment, and API endpoints  
- **TT Inference Server**: FastAPI server for handling model inference requests
- **Docker Containers**: Complete containerization for isolation and easy deployment
- **Automatic Hardware Detection**: Seamless integration and auto-mounting of Tenstorrent devices (/dev/tenstorrent)
- **Automated Setup**: Complete environment configuration and model setup automation via run.py script

## Supported AI Models

- **Chat-based Language Models (LLMs)**: Text generation and conversational AI
- **Computer Vision (YOLO)**: Object detection and image analysis
- **Speech Recognition (Whisper)**: Audio-to-text transcription
- **Image Generation (Stable Diffusion)**: AI-powered image creation
