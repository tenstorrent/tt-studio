# SPDX-License-Identifier: Apache-2.0
#
# SPDX-FileCopyrightText: Â© 2024 Tenstorrent AI ULC

services:
  tt_studio_backend:
    container_name: tt_studio_backend_api
    # hostnames must not contain underscores
    hostname: tt-studio-backend-api
    image: ghcr.io/tenstorrent/tt-studio/api:v0.0.0
    build: ./api
    # uncomment devices to use Tenstorrent hardware
    # devices:
      # mount all tenstorrent devices to backend container
      # - /dev/tenstorrent:/dev/tenstorrent
    # note that `network_mode: host` does not work on mac OS
    networks:
      - llm_studio_network
    ports:
      - "8000:8000"
    # command: bash
    # dev server can be used for breakpoint debugging, does not support streaming
    # command: ./manage.py runserver 0.0.0.0:8000
    # gunicorn is used from production, supports streaming
    command: gunicorn --workers 3 --bind 0.0.0.0:8000 --timeout 1200 api.wsgi:application
    stdin_open: true
    tty: true
    environment:
      # env vars are defined in .env file, use .env.default as template
      - TT_STUDIO_ROOT
      - HOST_PERSISTENT_STORAGE_VOLUME
      - INTERNAL_PERSISTENT_STORAGE_VOLUME
      - BACKEND_API_HOSTNAME
      - JWT_SECRET
    volumes:
      # mounting docker unix socket allows for backend container to run docker cmds
      - /var/run/docker.sock:/var/run/docker.sock
      - ${HOST_PERSISTENT_STORAGE_VOLUME}:${INTERNAL_PERSISTENT_STORAGE_VOLUME}
      # for development mount api changes
      - ./api:/api

  tt_studio_frontend:
    container_name: tt_studio_frontend
    hostname: tt-studio-frontend
    image: ghcr.io/tenstorrent/tt-studio/frontend:v0.0.0
    build: ./frontend
    networks:
      - llm_studio_network
    ports:
      - "3000:3000"
    volumes:
      # for development mount api changes
      - ./frontend:/frontend
    # command: sh
    command: bash -c "npm i && npm run dev"

networks:
  llm_studio_network:
    # need external flag to allow for the backend to manage the docker network
    # otherwise, docker compose will create an app_* network for the backend container
    # to avoid colliding existing docker networks
    external: true
    name: llm_studio_network
